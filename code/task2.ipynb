{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "import tensorflow\n",
    "import keras\n",
    "import pickle\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Embedding, Bidirectional, LSTM, Masking, Dense, Input, TimeDistributed, Activation, Lambda, Dropout\n",
    "from keras.optimizers import RMSprop\n",
    "#from keras_contrib.layers import CRF\n",
    "#from keras_contrib.losses import crf_loss\n",
    "#from keras_contrib.metrics import crf_viterbi_accuracy\n",
    "from keras import backend as K\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超参量\n",
    "SAVE_PATH = 'config.pkl'\n",
    "RNN_UNITS = 300\n",
    "BATCH_SIZE = 32\n",
    "EMBED_DIM = 300\n",
    "EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   0\n",
      "0  “  人们  常  说  生活  是  一  部  教科书  ，  而  血  与  火  ...\n",
      "1  “  心  静  渐  知  春  似  海  ，  花  深  每  觉  影  生  香  。\n",
      "2    “  吃  屎  的  东西  ，  连  一  捆  麦  也  铡  不  动  呀  ？\n",
      "3  他  “  严格要求  自己  ，  从  一个  科举  出身  的  进士  成为  一...\n",
      "4  “  征  而  未  用  的  耕地  和  有  收益  的  土地  ，  不准  ...\n",
      "                                                   0\n",
      "0                      扬帆  远东  做  与  中国  合作  的  先行  \n",
      "1                            希腊  的  经济  结构  较  特殊  。\n",
      "2  海运  业  雄踞  全球  之  首  ，  按  吨位  计  占  世界  总数  的...\n",
      "3  另外  旅游  、  侨汇  也是  经济  收入  的  重要  组成部分  ，  制造业...\n",
      "4  多年来  ，  中  希  贸易  始终  处于  较低  的  水平  ，  希腊  几乎...\n"
     ]
    }
   ],
   "source": [
    "# train_set = open('msrseg/msr_training.utf8', 'r', encoding='utf-8')\n",
    "train_set = pd.read_csv('msrseg/msr_training.utf8', encoding= 'utf8', header=None)  # 不把第一行作为列属性，且pd读出来就是数据帧，就是字符串\n",
    "test_set = pd.read_csv('msrseg/msr_test_gold.utf8', encoding='utf8', header=None)\n",
    "print(train_set.head())\n",
    "print(test_set.head())\n",
    "#train_set = train_set.values  #转成二维的nparray[[s1],[s2],]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将句子转换成词序列\n",
    "def get_word(sentence):\n",
    "    word_list = []\n",
    "    sentence = re.sub(\"[+\\.\\!\\/_,$%^*(+\\“\\”\\‘\\’]+|[+——！，。？、；：《》【】~@#￥%……&*（）]\", \"\",sentence)  # 去掉所有除空格外的标点符号\n",
    "    sentence = sentence.split() #去掉空格\n",
    "    # sentence.append('<EOS>')  # 加入结束符<EOS>\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file):\n",
    "    word, content = [], []\n",
    "    maxlen = 0\n",
    "\n",
    "    for i in range(len(file)):  # 记得加range！！\n",
    "        line = file.loc[i,0]   # 用loc来访问dataframe\n",
    "        line = line.strip('\\n') #去掉换行符\n",
    "        line = line.strip(' ')  #去掉开头和结尾的空格\n",
    "        \n",
    "        word_list = get_word(line)        #获得字列表：去掉标点，（不添加<EOS>结束符\n",
    "        \n",
    "        maxlen = max(maxlen, len(word_list))\n",
    "        word.extend(word_list)            #每一个单元是1个词，且不加<EOS>符号\n",
    "        content.append(word_list)         # 每一个单元是一行里面的各个词（分好）\n",
    "    return word, content, maxlen  #word是单列表，content和label是双层列表\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process data: padding\n",
    "def process_data(word_list, vocab, MAXLEN):\n",
    "    # vocab to idx dictionary:\n",
    "    vocab2idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "    # x: get every idx of every word, map to idx in vocab, set to <EOS> if not in vocab(<EOS> not included in vocab)\n",
    "    x = [[vocab2idx.get(word, 1) for word in s] for s in word_list]\n",
    "    \n",
    "    # y: get next word idx\n",
    "    y = []\n",
    "    for i in range(len(word_list)):\n",
    "        temp = []\n",
    "        for j in range(len(word_list[i])):\n",
    "            if j == len(word_list[i]) - 1:\n",
    "                temp.append(1)  # 1 means <EOS>\n",
    "            else:\n",
    "                temp.append(x[i][j+1])\n",
    "        y.append(temp)\n",
    "    \n",
    "    # padding of x, default is 0(symbolizes <PAD>). padding includes:over->cutoff, less->padding. default: left_padding\n",
    "    x = pad_sequences(x, maxlen=MAXLEN, value=0, padding='post', truncating='post')\n",
    "    # padding of y, default is 0. right padding\n",
    "    y = pad_sequences(y, maxlen=MAXLEN, value=0, padding='post', truncating='post')\n",
    "    # one-hot of y\n",
    "    y = to_categorical(y, len(vocab))\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    train_word, train_content, _ = read_file(train_set)\n",
    "    test_word, test_content, maxlen = read_file(test_set)\n",
    "    \n",
    "    vocab = list(set(train_word + test_word))   # 合并，构成大词表\n",
    "    special_chars = ['<PAD>', '<EOS>']   #特殊词表示：PAD表示padding，EOS表示句子结尾\n",
    "    vocab = special_chars + vocab\n",
    "    \n",
    "    # save initial config data\n",
    "    with open(SAVE_PATH, 'wb') as f:\n",
    "        pickle.dump((vocab), f)\n",
    "    \n",
    "    # process data: padding\n",
    "    print('maxlen is %d' % maxlen)\n",
    "    return train_content, test_content, vocab, maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model_path = 'sgns.wiki.word.bz2'  #词向量位置\n",
    "word2vec_model = KeyedVectors.load_word2vec_format(word2vec_model_path, binary=False, unicode_errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_embeddings_matrix(word2vec_model, vocab):\n",
    "    word2vec_dict = {}    # 字对字向量\n",
    "    vocab2idx = {char: idx for idx, char in enumerate(vocab)}\n",
    "    for word, vector in zip(word2vec_model.vocab, word2vec_model.vectors):\n",
    "        word2vec_dict[word] = vector\n",
    "    embeddings_matrix = np.zeros((len(vocab), EMBED_DIM))# form huge matrix\n",
    "    for i in tqdm(range(2, len(vocab))):\n",
    "        word = vocab[i]\n",
    "        if word in word2vec_dict.keys():    # 如果word在词向量列表中，更新权重；否则，赋值为全0（默认）\n",
    "            word_vector = word2vec_dict[word]\n",
    "            embeddings_matrix[i] = word_vector\n",
    "    return embeddings_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_content, test_content, vocab, maxlen = load_data()\n",
    "# change maxlen\n",
    "maxlen = 50\n",
    "embeddings_matrix = make_embeddings_matrix(word2vec_model, vocab)\n",
    "# input layer\n",
    "inputs = Input(shape=(maxlen, ), dtype='int32')\n",
    "# masking layer 屏蔽层\n",
    "# x = Masking(mask_value=0)(inputs)\n",
    "# embedding layer: map the word to it's weights(with embedding-matrix)\n",
    "x = Embedding(len(vocab), EMBED_DIM, weights=[embeddings_matrix], input_length=maxlen, trainable=True)(inputs)\n",
    "# LSTM layer\n",
    "x = LSTM(RNN_UNITS, input_shape=(maxlen, EMBED_DIM), return_sequences=True)(x)\n",
    "# Dropout: 正则化，防止过拟合.argument means percentage\n",
    "# x = Dropout(0.5)(x)\n",
    "# 一维展开，全连接\n",
    "x = TimeDistributed(Dense(len(vocab)))(x)\n",
    "# 激活函数：softmax\n",
    "outputs = Activation('softmax')(x)\n",
    "# model\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "# print arguments of each layer\n",
    "model.summary()\n",
    "# target_function: includes optimizer, function_type, metrics\n",
    "RMSPROP = keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_test_data(word_idx, maxlen, index):\n",
    "    result = [0] * maxlen\n",
    "    result[index] = word_idx\n",
    "    result = np.array(result) \n",
    "    # print(result.shape)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "def train(start1, end1, epochs1, batch_size1):\n",
    "    model.load_weights('model1.h5')\n",
    "    maxlen = 50   # maxlen取50，直接截断\n",
    "    start = start1\n",
    "    EPOCHS = epochs1\n",
    "    TRAIN_BATCH = 200\n",
    "    while start < len(train_content):\n",
    "        print(start)\n",
    "        file = open('temp1.txt', 'a')\n",
    "        file.write(str(start))\n",
    "        file.write('\\n')\n",
    "        file.close()\n",
    "        if start == end1:\n",
    "            break\n",
    "        if start % 10000 == 0:\n",
    "            model.save_weights('model1.h5')\n",
    "        if start+TRAIN_BATCH <= len(train_content):\n",
    "            train_x, train_y = process_data(train_content[start: start+TRAIN_BATCH], vocab, maxlen)\n",
    "        else:\n",
    "            train_x, train_y = process_data(train_content[start: ], vocab, maxlen)\n",
    "        \n",
    "        model.fit(train_x, train_y, batch_size=batch_size1, epochs=EPOCHS, verbose=0, validation_split=0.1)\n",
    "        start += TRAIN_BATCH\n",
    "    model.save_weights('model1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab2idx = {word: idx for idx, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test1(start, test_num1):\n",
    "    model.load_weights('model1.h5')\n",
    "    file = open('result1.txt', 'a')\n",
    "    \n",
    "    i = 0\n",
    "    j = 0\n",
    "    TEST_NUM = test_num1\n",
    "    for i in range(start, start + TEST_NUM):\n",
    "        if len(test_content[i])==0:\n",
    "            continue\n",
    "        sentence = [test_content[i][0]]\n",
    "        word_idx = vocab2idx.get(test_content[i][0])\n",
    "        test_x = build_test_data(word_idx, maxlen, 0)\n",
    "        for j in range(0, 49):\n",
    "            temp = []\n",
    "            temp.append(test_x)\n",
    "            temp = np.array(temp)\n",
    "            next_word = model.predict(temp, batch_size=1)  # 输入得是numpy数组，不能是list\n",
    "            index = np.argmax(next_word[0][j])\n",
    "            if index == 1 or index == 0:   # means predict <EOS>\n",
    "                print(i,j, index)\n",
    "                file.write(str(i))\n",
    "                file.write(' ')\n",
    "                file.write(str(j))\n",
    "                file.write(' ')\n",
    "                file.write(str(index))\n",
    "                file.write('\\n')\n",
    "                break\n",
    "            sentence.append(vocab[index])\n",
    "            test_x = build_test_data(index, maxlen, j+1)\n",
    "        for word in sentence:\n",
    "            file.write(word)\n",
    "            file.write(' ')\n",
    "        file.write('<EOS>')\n",
    "        file.write('\\n')\n",
    "        print(sentence)   \n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(start1=0, end1=90000, epochs1=15, batch_size1=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1(start=0, test_num1=3985)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
